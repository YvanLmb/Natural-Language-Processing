{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chapitre 4   \n\n\nDans ce TP, nous allons effectuer le finetuning d'un mod√®le transformer pr√©-entra√Æn√© (BERT) sur une des t√¢ches du benchmark GLUE.  \n\nLe benchmark [GLUE](https://gluebenchmark.com/) (General Language Understanding Evaluation) est un ensemble de dataset pour l'apprentissage et l'√©valuation des mod√®les NLP. Il contient plusieurs \"t√¢ches\" avec un dataset train sur lequel on peut entra√Æner le mod√®le et un dataset test pour l'√©valuer. Cette diversit√© de t√¢ches permet d'appr√©cier la capacit√© de compr√©hension du langage du mod√®le.  \n\nNous utiliserons la library datasets de huggingface qui contient les datasets pour chacune des t√¢ches du benchmark : [glue](https://huggingface.co/datasets/glue). La description des t√¢ches est sp√©cifi√©e dans la page du dataset.\n\nVous √™tes libre de choisir la t√¢che sur laquelle vous souhaitez travailler. Par d√©faut, ce notebook suppose que la t√¢che choisie est la t√¢che MRPC (Microsoft Research Paraphrase Corpus) qui consiste √† pr√©dire si des paires de phrases sont s√©mantiquement √©quivalentes.  \n\nPour ce TP, nous utiliserons la library transformers et torch pour effectuer l'entra√Ænement du mod√®le.  ","metadata":{"id":"l9Rj_bkfUfTR"}},{"cell_type":"markdown","source":"Chargez le dataset Glue pour la t√¢che MRPC","metadata":{"id":"IHALjy6YWUGd"}},{"cell_type":"code","source":"!pip install transformers datasets torch\nfrom datasets import load_dataset\n\n# Charger le dataset MRPC\ndataset = load_dataset(\"glue\", \"mrpc\")\nprint(dataset)","metadata":{"id":"51GhILSNUaWK","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:30:58.086205Z","iopub.execute_input":"2024-11-20T15:30:58.086870Z","iopub.status.idle":"2024-11-20T15:31:08.406043Z","shell.execute_reply.started":"2024-11-20T15:30:58.086833Z","shell.execute_reply":"2024-11-20T15:31:08.404833Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})\n","output_type":"stream"}],"execution_count":326},{"cell_type":"markdown","source":"Chargez le mod√®le pr√©-entra√Æn√© BERT (bert-base-uncased) ainsi que son tokenizer.\n\nUtilisez la classe AutoModelForSequenceClassification ou BertModelForSequenceClassification.","metadata":{"id":"UF2U-Q1YWhfB"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\n\n# Charger le tokenizer pour le mod√®le bert-base-uncased\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n# Charger le mod√®le BERT pr√©-entra√Æn√© pour une t√¢che de classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n# Texte exemple\nsentence1 = \"The cat sat on the mat.\"\nsentence2 = \"The feline was resting on the rug.\"\n\n# Tokenisation\ninputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\", padding=True, truncation=True)\n\n# Passage dans le mod√®le\noutputs = model(**inputs)\n\n# R√©sultat des logits\nprint(outputs.logits)","metadata":{"id":"bV9n7kqFXBzM","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.407835Z","iopub.execute_input":"2024-11-20T15:31:08.408162Z","iopub.status.idle":"2024-11-20T15:31:08.786418Z","shell.execute_reply.started":"2024-11-20T15:31:08.408129Z","shell.execute_reply":"2024-11-20T15:31:08.785573Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"tensor([[0.0564, 0.4043]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}],"execution_count":327},{"cell_type":"markdown","source":"Affichez quelques informations de base sur le mod√®le et son tokenizer pour. Exemples:  \n- Nombre de param√®tres  \n- Architecture du mod√®le  \n- Taille du vocabulaire  \n- ...","metadata":{"id":"cGR0wFTZtIKE"}},{"cell_type":"markdown","source":"Affichez quelques exemples du dataset","metadata":{"id":"XPQO3QHLsn6L"}},{"cell_type":"code","source":"print(\"Nombre de param√®tres :\", model.num_parameters())\nprint(\"Architecture du mod√®le :\", model.config.architectures)\nprint(\"Nombre de couches :\", model.config.num_hidden_layers)\nprint(\"Taille des embeddings :\", model.config.hidden_size)\nprint(\"Taille du vocabulaire :\", tokenizer.vocab_size)\nprint(\"Token sp√©cial [CLS] :\", tokenizer.cls_token)\nprint(\"Token sp√©cial [SEP] :\", tokenizer.sep_token)\nprint(\"Token sp√©cial [PAD] :\", tokenizer.pad_token)\n\nfor i in range(3):\n    print(f\"Exemple {i + 1}:\")\n    print(\"Phrase 1 :\", dataset['train'][i]['sentence1'])\n    print(\"Phrase 2 :\", dataset['train'][i]['sentence2'])\n    print(\"Label :\", dataset['train'][i]['label'])","metadata":{"id":"pP8uZYcZsr8_","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.788001Z","iopub.execute_input":"2024-11-20T15:31:08.788421Z","iopub.status.idle":"2024-11-20T15:31:08.800217Z","shell.execute_reply.started":"2024-11-20T15:31:08.788376Z","shell.execute_reply":"2024-11-20T15:31:08.798894Z"}},"outputs":[{"name":"stdout","text":"Nombre de param√®tres : 109483778\nArchitecture du mod√®le : ['BertForMaskedLM']\nNombre de couches : 12\nTaille des embeddings : 768\nTaille du vocabulaire : 30522\nToken sp√©cial [CLS] : [CLS]\nToken sp√©cial [SEP] : [SEP]\nToken sp√©cial [PAD] : [PAD]\nExemple 1:\nPhrase 1 : Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\nPhrase 2 : Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\nLabel : 1\nExemple 2:\nPhrase 1 : Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\nPhrase 2 : Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\nLabel : 0\nExemple 3:\nPhrase 1 : They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\nPhrase 2 : On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\nLabel : 1\n","output_type":"stream"}],"execution_count":328},{"cell_type":"markdown","source":"Tokenisez quelques exemples du dataset et interpretez l'output de la fonction de tokenisation","metadata":{"id":"QcTBTmuMXCBV"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# Tokeniser quelques exemples\nfor i in range(3):\n    example = dataset['train'][i]\n    tokens = tokenizer(example['sentence1'], example['sentence2'], truncation=True, padding=True, return_tensors=\"pt\",max_length=65)\n    \n    # Affichage des informations\n    print(f\"Exemple {i + 1}:\")\n    print(\"Phrase 1 :\", example['sentence1'])\n    print(\"Phrase 2 :\", example['sentence2'])\n    print(\"Tokens :\", tokens['input_ids'])\n    print(\"Attention Mask :\", tokens['attention_mask'])\n    print(\"Token Type IDs :\", tokens['token_type_ids'])\n    print(\"-\" * 50)\n","metadata":{"id":"Iq13elJEXj4D","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.802795Z","iopub.execute_input":"2024-11-20T15:31:08.803106Z","iopub.status.idle":"2024-11-20T15:31:08.819868Z","shell.execute_reply.started":"2024-11-20T15:31:08.803072Z","shell.execute_reply":"2024-11-20T15:31:08.818931Z"}},"outputs":[{"name":"stdout","text":"Exemple 1:\nPhrase 1 : Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\nPhrase 2 : Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\nTokens : tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]])\nAttention Mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]])\nToken Type IDs : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]])\n--------------------------------------------------\nExemple 2:\nPhrase 1 : Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\nPhrase 2 : Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\nTokens : tensor([[  101,  9805,  3540, 11514,  2050,  3079, 11282,  2243,  1005,  1055,\n          2077,  4855,  1996,  4677,  2000,  3647,  4576,  1999,  2687,  2005,\n          1002,  1016,  1012,  1019,  4551,  1012,   102,  9805,  3540, 11514,\n          2050,  4149, 11282,  2243,  1005,  1055,  1999,  2786,  2005,  1002,\n          6353,  2509,  2454,  1998,  2853,  2009,  2000,  3647,  4576,  2005,\n          1002,  1015,  1012,  1022,  4551,  1999,  2687,  1012,   102]])\nAttention Mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\nToken Type IDs : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\nExemple 3:\nPhrase 1 : They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\nPhrase 2 : On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\nTokens : tensor([[  101,  2027,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  2006,\n          2238,  2184,  1010,  5378,  1996,  6636,  2005,  5096,  1010,  2002,\n          2794,  1012,   102,  2006,  2238,  2184,  1010,  1996,  2911,  1005,\n          1055,  5608,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  1010,\n          5378,  1996, 14792,  2005,  5096,  1012,   102]])\nAttention Mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\nToken Type IDs : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":329},{"cell_type":"markdown","source":"Utilisez la fonction decode du tokenizer afin d'avoir une repr√©sentation plus lisible du r√©sultat de tokenisation. Vous remarquerez l'ajout des tokens [CLS], [SEP] notamment.","metadata":{"id":"hjUYsmhjXkFw"}},{"cell_type":"code","source":"# Tokeniser quelques exemples du dataset\ntokenized_example_1 = tokenizer(\n    \"Amrozi accused his brother, whom he called 'the witness', of deliberately distorting his evidence.\",\n    padding='max_length', truncation=True, return_tensors='pt'\n)\ntokenized_example_2 = tokenizer(\n    \"Referring to him as only 'the witness', Amrozi accused his brother of deliberately distorting his evidence.\",\n    padding='max_length', truncation=True, return_tensors='pt'\n)\n\n# Utilisation de la fonction decode pour afficher une repr√©sentation lisible\ndecoded_example_1 = tokenizer.decode(\n    tokenized_example_1[\"input_ids\"][0], skip_special_tokens=False\n)\ndecoded_example_2 = tokenizer.decode(\n    tokenized_example_2[\"input_ids\"][0], skip_special_tokens=False\n)\n\nprint(\"Phrase tokenis√©e et d√©cod√©e - Exemple 1 :\")\nprint(decoded_example_1)\nprint(\"\\nPhrase tokenis√©e et d√©cod√©e - Exemple 2 :\")\nprint(decoded_example_2)\n","metadata":{"id":"_cvcdwSXX6Bt","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.821102Z","iopub.execute_input":"2024-11-20T15:31:08.821434Z","iopub.status.idle":"2024-11-20T15:31:08.834066Z","shell.execute_reply.started":"2024-11-20T15:31:08.821401Z","shell.execute_reply":"2024-11-20T15:31:08.833116Z"}},"outputs":[{"name":"stdout","text":"Phrase tokenis√©e et d√©cod√©e - Exemple 1 :\n[CLS] amrozi accused his brother, whom he called ' the witness ', of deliberately distorting his evidence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\nPhrase tokenis√©e et d√©cod√©e - Exemple 2 :\n[CLS] referring to him as only ' the witness ', amrozi accused his brother of deliberately distorting his evidence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","output_type":"stream"}],"execution_count":330},{"cell_type":"markdown","source":"Tokenisez tout le dataset.\n\nConseil : Utilisez la fonction map de la library datasets.","metadata":{"id":"7Rr1jeb9X6Ok"}},{"cell_type":"code","source":"# Tokenisation de tout le dataset avec la fonction map\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=True, return_tensors=\"pt\",max_length=65)\n\n# Application de la fonction de tokenisation\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n# Affichage des cl√©s du dataset tokenis√© pour confirmation\nprint(tokenized_dataset)\n","metadata":{"id":"npPFpk2DY4jr","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.835032Z","iopub.execute_input":"2024-11-20T15:31:08.835329Z","iopub.status.idle":"2024-11-20T15:31:08.877460Z","shell.execute_reply.started":"2024-11-20T15:31:08.835297Z","shell.execute_reply":"2024-11-20T15:31:08.876542Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1725\n    })\n})\n","output_type":"stream"}],"execution_count":331},{"cell_type":"markdown","source":"Une fois le dataset tokenis√©, il faut cr√©er un DataLoader qui va donner les exemples au mod√®le durant l'entra√Ænement.   \nD√©finissez un train_dataloader et un eval_dataloader.  \n\nPour le train_dataloader, sp√©cifiez `shuffle=True` afin que les exemples soient donn√©s dans un ordre al√©atoire au mod√®le.\n\nIl est √† noter que le mod√®le s'attend √† prendre en entr√©e des exemples sous forme de **dictionnaires** de **tenseurs torch** et ayant les cl√©s suivantes uniquement:  \n- attention_mask  \n- input_ids  \n- labels  \n- token_type_ids  \n\nVous aurez probablement √† d√©finir une fonction de collate √† donner en param√®tre √† vos dataloaders dans le param√®tre `collate_fn`.  Cette fonction est appel√©e par le dataloader juste avant de donner l'exemple au mod√®le afin de lui donner le bon format (mettre uniquement les cl√©s attendues par le mod√®le, padder les exemples √† une longueur fixe, ...).  \n\n- [How to use collate_fn](https://discuss.pytorch.org/t/how-to-use-collate-fn/27181)  \n- [How to use 'collate_fn' with dataloaders?](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)  ","metadata":{"id":"UO0-1VvJY1uG"}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch\n\n# 1. Dataset personnalis√©\nclass CustomDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.tokenized_data = tokenized_data  # Votre dataset tokenis√©\n\n    def __len__(self):\n        return len(self.tokenized_data)\n\n    def __getitem__(self, idx):\n        return self.tokenized_data[idx]\n        \n# S√©parer le dataset en train et √©valuation\ntrain_dataset = tokenized_dataset['train']  # Donn√©es d'entra√Ænement\neval_dataset = tokenized_dataset['validation']  # Donn√©es de validation\n\n# 2. Fonction collate_fn\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    # D√©finir la longueur maximale pour le padding\n    max_len = 65  # La longueur maximale que vous souhaitez pour les s√©quences\n    \n    input_ids = [torch.tensor(item['input_ids'][:max_len]) for item in batch]\n    attention_masks = [torch.tensor(item['attention_mask'][:max_len]) for item in batch]\n    token_type_ids = [torch.tensor(item['token_type_ids'][:max_len]) for item in batch]\n    \n    labels = [torch.tensor(item['label']) for item in batch]\n\n    # Padding √† la longueur maximale\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n    token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n    \n    batch_dict = {\n        'input_ids': input_ids,\n        'attention_mask': attention_masks,\n        'labels': labels,\n        'token_type_ids': token_type_ids\n\n    }\n    \n    if labels is not None:\n        batch_dict['labels'] = torch.stack(labels)\n\n    return batch_dict\n\n\n\n\n# 3. Charger votre dataset tokenis√© (supposons que vous avez `train_dataset` et `eval_dataset` d√©j√† d√©finis)\ntrain_dataset = CustomDataset(train_dataset)  # Votre dataset tokenis√© pour l'entra√Ænement\neval_dataset = CustomDataset(eval_dataset)    # Votre dataset tokenis√© pour l'√©valuation\n\n# 4. Cr√©er les DataLoader\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=True,\n    collate_fn=collate_fn\n)\n\neval_dataloader = DataLoader(\n    eval_dataset,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=collate_fn\n)\n","metadata":{"id":"c26Wa7-OZh2T","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.878875Z","iopub.execute_input":"2024-11-20T15:31:08.879195Z","iopub.status.idle":"2024-11-20T15:31:08.895640Z","shell.execute_reply.started":"2024-11-20T15:31:08.879161Z","shell.execute_reply":"2024-11-20T15:31:08.894441Z"}},"outputs":[],"execution_count":332},{"cell_type":"markdown","source":"Afin de v√©rifier que votre Dataloader fonctionne bien, vous pouvez it√©rer dessus comme ci-dessous :","metadata":{"id":"ynCNm7VeY54_"}},{"cell_type":"code","source":"for batch in train_dataloader:\n    break\n{k: v.shape for k, v in batch.items()}","metadata":{"id":"f5mISj69c07Z","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.897117Z","iopub.execute_input":"2024-11-20T15:31:08.897422Z","iopub.status.idle":"2024-11-20T15:31:08.917247Z","shell.execute_reply.started":"2024-11-20T15:31:08.897389Z","shell.execute_reply":"2024-11-20T15:31:08.916226Z"}},"outputs":[{"execution_count":333,"output_type":"execute_result","data":{"text/plain":"{'input_ids': torch.Size([8, 65]),\n 'attention_mask': torch.Size([8, 65]),\n 'labels': torch.Size([8]),\n 'token_type_ids': torch.Size([8, 65])}"},"metadata":{}}],"execution_count":333},{"cell_type":"markdown","source":"Le code ne devrait pas crasher et vous devriez avoir une output similaire √†:  \n```\n{\n  'attention_mask': torch.Size([8, 65]),\n  'input_ids': torch.Size([8, 65]),\n  'labels': torch.Size([8]),\n  'token_type_ids': torch.Size([8, 65])\n}\n```","metadata":{"id":"_jUpAq0QdBF8"}},{"cell_type":"markdown","source":"Vous pouvez vous assurer que le mod√®le prend en entr√©e les donn√©es du dataloader en lui donnant en entr√©e le batch pr√©c√©dent. Voici un exemple de code:  ","metadata":{"id":"fkBg5LLJdjoi"}},{"cell_type":"code","source":"outputs = model(**batch)\nprint(outputs.loss, outputs.logits.shape)","metadata":{"id":"Xt5VbJ_dd2Lc","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.918416Z","iopub.execute_input":"2024-11-20T15:31:08.918721Z","iopub.status.idle":"2024-11-20T15:31:09.521225Z","shell.execute_reply.started":"2024-11-20T15:31:08.918690Z","shell.execute_reply":"2024-11-20T15:31:09.520243Z"}},"outputs":[{"name":"stdout","text":"tensor(0.4949, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n","output_type":"stream"}],"execution_count":334},{"cell_type":"markdown","source":"Maintenant que tout est en place, nous allons lancer l'entra√Ænement.  \nD√©finissez l'optimizer √† utiliser durant l'apprentissage. Par exemple AdamW.  \nD√©finissez aussi les param√®tres d'apprentissage, par exemple:  \n- Nombre d'epochs  \n- Nombre de training steps  \n- Un schedule pour le learning rate ou un learning rate fixe  ","metadata":{"id":"yPSrtCtsd57K"}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# D√©finir l'optimiseur\noptimizer = AdamW(model.parameters(), lr=2e-5)\n# Nombre d'√©poques et de training steps\nnum_epochs = 3\nnum_training_steps = len(train_dataloader) * num_epochs\n\n\n# D√©finir un scheduler de learning rate avec warm-up\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,  # Nombre d'√©tapes pour le warm-up (souvent entre 0 et 10% des √©tapes totales)\n    num_training_steps=num_training_steps\n)\n","metadata":{"id":"7tPpK-lueeIA","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:09.524279Z","iopub.execute_input":"2024-11-20T15:31:09.524614Z","iopub.status.idle":"2024-11-20T15:31:09.530858Z","shell.execute_reply.started":"2024-11-20T15:31:09.524587Z","shell.execute_reply":"2024-11-20T15:31:09.529812Z"}},"outputs":[],"execution_count":335},{"cell_type":"markdown","source":"\nVous pouvez aussi utiliser le Trainer de la library transformers.  ","metadata":{"id":"V8ISzCW9edYX"}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score\n\n# D√©finir la fonction de calcul des m√©triques\ndef compute_metrics(p):\n    preds = p.predictions.argmax(axis=1)  # Pr√©dictions en prenant l'index avec la probabilit√© la plus √©lev√©e\n    labels = p.label_ids\n    return {\"eval_accuracy\": accuracy_score(labels, preds)}  # Renvoie eval_accuracy comme cl√©\n\n# D√©finir les arguments d'entra√Ænement\ntraining_args = TrainingArguments(\n    output_dir='./results',             # R√©pertoire pour sauvegarder les r√©sultats\n    num_train_epochs=3,                 # Nombre d'√©poques\n    per_device_train_batch_size=8,      # Taille du batch pour l'entra√Ænement\n    per_device_eval_batch_size=8,       # Taille du batch pour l'√©valuation\n    warmup_steps=0,                     # Nombre de warm-up steps\n    weight_decay=0.01,                  # Poids pour la r√©gularisation L2\n    logging_dir='./logs',               # R√©pertoire des logs\n    logging_steps=10,                   # Fr√©quence de logging\n    evaluation_strategy=\"epoch\",        # √âvaluation apr√®s chaque √©poque\n    save_strategy=\"epoch\",              # Sauvegarde apr√®s chaque √©poque\n    load_best_model_at_end=True,        # Charger le meilleur mod√®le √† la fin\n    metric_for_best_model=\"eval_accuracy\",  # Crit√®re pour s√©lectionner le meilleur mod√®le\n    report_to=\"none\"                   # Pas de rapport vers W&B\n)\n\n# Cr√©er un objet Trainer\ntrainer = Trainer(\n    model=model,                        # Mod√®le √† entra√Æner\n    args=training_args,                 # Les arguments d'entra√Ænement\n    train_dataset=train_dataset,         # Dataset d'entra√Ænement\n    eval_dataset=eval_dataset,           # Dataset d'√©valuation\n    tokenizer=tokenizer,                 # Tokenizer\n    compute_metrics=compute_metrics,     # Fonction pour calculer les m√©triques\n    optimizers=(optimizer, lr_scheduler) # Optimiseur et scheduler\n)\n\n# Lancer l'entra√Ænement\ntrainer.train()\n\n","metadata":{"id":"wlB1fr9feo9K","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:09.532186Z","iopub.execute_input":"2024-11-20T15:31:09.532872Z","iopub.status.idle":"2024-11-20T15:33:19.099243Z","shell.execute_reply.started":"2024-11-20T15:31:09.532831Z","shell.execute_reply":"2024-11-20T15:33:19.098325Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1377/1377 02:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.561500</td>\n      <td>0.368206</td>\n      <td>0.843137</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.238100</td>\n      <td>0.400536</td>\n      <td>0.860294</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.026400</td>\n      <td>0.518481</td>\n      <td>0.867647</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":336,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1377, training_loss=0.32814860387218975, metrics={'train_runtime': 128.8769, 'train_samples_per_second': 85.384, 'train_steps_per_second': 10.685, 'total_flos': 367564088782800.0, 'train_loss': 0.32814860387218975, 'epoch': 3.0})"},"metadata":{}}],"execution_count":336},{"cell_type":"markdown","source":"Ecrivez la boucle d'√©valuation de votre mod√®le. Pour cela, vous pouvez utiliser la library evaluate de huggingface qui facilite le calcul des m√©triques. Elle contient la m√©trique sp√©cifique √† la t√¢che qui nous int√©resse. Voici un exemple de comment la charger :  \n\nVoici la page associ√©e pour avoir de la doc sur les m√©triques GLUE : [Metric: glue](https://huggingface.co/spaces/evaluate-metric/glue)","metadata":{"id":"oZZtN8L8ZjtJ"}},{"cell_type":"code","source":"import torch\nimport evaluate\nfrom tqdm import tqdm\n\n# Charger la m√©trique (par exemple, accuracy)\naccuracy_metric = evaluate.load(\"accuracy\")\n\n# D√©finir le p√©riph√©rique (GPU ou CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Fonction d'√©valuation\ndef evaluate_model(model, dataloader, device):\n    model.eval()  # Mettre le mod√®le en mode √©valuation\n    all_preds = []\n    all_labels = []\n    \n    # D√©placer le mod√®le vers le bon p√©riph√©rique\n    model.to(device)\n    \n    # D√©sactivation de la mise √† jour des gradients\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            # D√©placer les donn√©es vers le bon p√©riph√©rique (GPU ou CPU)\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Effectuer une passe avant (forward pass)\n            outputs = model(**batch)\n            logits = outputs.logits\n            \n            # Convertir les logits en pr√©dictions de classes\n            preds = torch.argmax(logits, dim=-1)\n            \n            # Ajouter les pr√©dictions et labels √† leurs listes respectives\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n    \n    # Calcul de la m√©trique (ex. : accuracy)\n    results = accuracy_metric.compute(predictions=all_preds, references=all_labels)\n    \n    return results\n\n# Appel de la fonction d'√©valuation sur votre mod√®le\nresults = evaluate_model(model, eval_dataloader, device)\nprint(\"Evaluation Results:\", results)\n","metadata":{"id":"MB-Nq-cufbQU","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:38:56.346900Z","iopub.execute_input":"2024-11-20T15:38:56.347711Z","iopub.status.idle":"2024-11-20T15:38:57.744673Z","shell.execute_reply.started":"2024-11-20T15:38:56.347675Z","shell.execute_reply":"2024-11-20T15:38:57.743806Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:01<00:00, 45.31it/s]","output_type":"stream"},{"name":"stdout","text":"Evaluation Results: {'accuracy': 0.8676470588235294}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":341},{"cell_type":"markdown","source":"# [Optionnel] G√©n√©ration de texte avec GPT2\n\nSi vous souhaitez exp√©rimenter avec un language model est faire de la g√©n√©ration de texte. Vous pouvez utiliser le mod√®le GPT 2 qui est un language model causal. Il permet donc d'effectuer de la g√©n√©ration de texte contrairement √† BERT qui est un mod√®le bidirectionnel et qui n'est pas fait pour cela.  \n\nVous pouvez essayer diff√©rents param√®tres de g√©n√©ration et algorithme de d√©codage en vous basant sur le tutoriel/guide suivant : [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate).","metadata":{"id":"bzTmAOFeuM7X"}},{"cell_type":"code","source":"","metadata":{"id":"4HR0z0M3vG0A","trusted":true},"outputs":[],"execution_count":null}]}
