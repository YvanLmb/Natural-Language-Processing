{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chapitre 4   \n\n\nDans ce TP, nous allons effectuer le finetuning d'un modèle transformer pré-entraîné (BERT) sur une des tâches du benchmark GLUE.  \n\nLe benchmark [GLUE](https://gluebenchmark.com/) (General Language Understanding Evaluation) est un ensemble de dataset pour l'apprentissage et l'évaluation des modèles NLP. Il contient plusieurs \"tâches\" avec un dataset train sur lequel on peut entraîner le modèle et un dataset test pour l'évaluer. Cette diversité de tâches permet d'apprécier la capacité de compréhension du langage du modèle.  \n\nNous utiliserons la library datasets de huggingface qui contient les datasets pour chacune des tâches du benchmark : [glue](https://huggingface.co/datasets/glue). La description des tâches est spécifiée dans la page du dataset.\n\nVous êtes libre de choisir la tâche sur laquelle vous souhaitez travailler. Par défaut, ce notebook suppose que la tâche choisie est la tâche MRPC (Microsoft Research Paraphrase Corpus) qui consiste à prédire si des paires de phrases sont sémantiquement équivalentes.  \n\nPour ce TP, nous utiliserons la library transformers et torch pour effectuer l'entraînement du modèle.  ","metadata":{"id":"l9Rj_bkfUfTR"}},{"cell_type":"markdown","source":"Chargez le dataset Glue pour la tâche MRPC","metadata":{"id":"IHALjy6YWUGd"}},{"cell_type":"code","source":"!pip install transformers datasets torch\nfrom datasets import load_dataset\n\n# Charger le dataset MRPC\ndataset = load_dataset(\"glue\", \"mrpc\")\nprint(dataset)","metadata":{"id":"51GhILSNUaWK","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:30:58.086205Z","iopub.execute_input":"2024-11-20T15:30:58.086870Z","iopub.status.idle":"2024-11-20T15:31:08.406043Z","shell.execute_reply.started":"2024-11-20T15:30:58.086833Z","shell.execute_reply":"2024-11-20T15:31:08.404833Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})\n","output_type":"stream"}],"execution_count":326},{"cell_type":"markdown","source":"Chargez le modèle pré-entraîné BERT (bert-base-uncased) ainsi que son tokenizer.\n\nUtilisez la classe AutoModelForSequenceClassification ou BertModelForSequenceClassification.","metadata":{"id":"UF2U-Q1YWhfB"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\n\n# Charger le tokenizer pour le modèle bert-base-uncased\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n# Charger le modèle BERT pré-entraîné pour une tâche de classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n# Texte exemple\nsentence1 = \"The cat sat on the mat.\"\nsentence2 = \"The feline was resting on the rug.\"\n\n# Tokenisation\ninputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\", padding=True, truncation=True)\n\n# Passage dans le modèle\noutputs = model(**inputs)\n\n# Résultat des logits\nprint(outputs.logits)","metadata":{"id":"bV9n7kqFXBzM","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.407835Z","iopub.execute_input":"2024-11-20T15:31:08.408162Z","iopub.status.idle":"2024-11-20T15:31:08.786418Z","shell.execute_reply.started":"2024-11-20T15:31:08.408129Z","shell.execute_reply":"2024-11-20T15:31:08.785573Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"tensor([[0.0564, 0.4043]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}],"execution_count":327},{"cell_type":"markdown","source":"Affichez quelques informations de base sur le modèle et son tokenizer pour. Exemples:  \n- Nombre de paramètres  \n- Architecture du modèle  \n- Taille du vocabulaire  \n- ...","metadata":{"id":"cGR0wFTZtIKE"}},{"cell_type":"markdown","source":"Affichez quelques exemples du dataset","metadata":{"id":"XPQO3QHLsn6L"}},{"cell_type":"code","source":"print(\"Nombre de paramètres :\", model.num_parameters())\nprint(\"Architecture du modèle :\", model.config.architectures)\nprint(\"Nombre de couches :\", model.config.num_hidden_layers)\nprint(\"Taille des embeddings :\", model.config.hidden_size)\nprint(\"Taille du vocabulaire :\", tokenizer.vocab_size)\nprint(\"Token spécial [CLS] :\", tokenizer.cls_token)\nprint(\"Token spécial [SEP] :\", tokenizer.sep_token)\nprint(\"Token spécial [PAD] :\", tokenizer.pad_token)\n\nfor i in range(3):\n    print(f\"Exemple {i + 1}:\")\n    print(\"Phrase 1 :\", dataset['train'][i]['sentence1'])\n    print(\"Phrase 2 :\", dataset['train'][i]['sentence2'])\n    print(\"Label :\", dataset['train'][i]['label'])","metadata":{"id":"pP8uZYcZsr8_","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.788001Z","iopub.execute_input":"2024-11-20T15:31:08.788421Z","iopub.status.idle":"2024-11-20T15:31:08.800217Z","shell.execute_reply.started":"2024-11-20T15:31:08.788376Z","shell.execute_reply":"2024-11-20T15:31:08.798894Z"}},"outputs":[{"name":"stdout","text":"Nombre de paramètres : 109483778\nArchitecture du modèle : ['BertForMaskedLM']\nNombre de couches : 12\nTaille des embeddings : 768\nTaille du vocabulaire : 30522\nToken spécial [CLS] : [CLS]\nToken spécial [SEP] : [SEP]\nToken spécial [PAD] : [PAD]\nExemple 1:\nPhrase 1 : Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\nPhrase 2 : Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\nLabel : 1\nExemple 2:\nPhrase 1 : Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\nPhrase 2 : Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\nLabel : 0\nExemple 3:\nPhrase 1 : They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\nPhrase 2 : On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\nLabel : 1\n","output_type":"stream"}],"execution_count":328},{"cell_type":"markdown","source":"Tokenisez quelques exemples du dataset et interpretez l'output de la fonction de tokenisation","metadata":{"id":"QcTBTmuMXCBV"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# Tokeniser quelques exemples\nfor i in range(3):\n    example = dataset['train'][i]\n    tokens = tokenizer(example['sentence1'], example['sentence2'], truncation=True, padding=True, return_tensors=\"pt\",max_length=65)\n    \n    # Affichage des informations\n    print(f\"Exemple {i + 1}:\")\n    print(\"Phrase 1 :\", example['sentence1'])\n    print(\"Phrase 2 :\", example['sentence2'])\n    print(\"Tokens :\", tokens['input_ids'])\n    print(\"Attention Mask :\", tokens['attention_mask'])\n    print(\"Token Type IDs :\", tokens['token_type_ids'])\n    print(\"-\" * 50)\n","metadata":{"id":"Iq13elJEXj4D","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.802795Z","iopub.execute_input":"2024-11-20T15:31:08.803106Z","iopub.status.idle":"2024-11-20T15:31:08.819868Z","shell.execute_reply.started":"2024-11-20T15:31:08.803072Z","shell.execute_reply":"2024-11-20T15:31:08.818931Z"}},"outputs":[{"name":"stdout","text":"Exemple 1:\nPhrase 1 : Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\nPhrase 2 : Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\nTokens : tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]])\nAttention Mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]])\nToken Type IDs : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1]])\n--------------------------------------------------\nExemple 2:\nPhrase 1 : Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\nPhrase 2 : Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\nTokens : tensor([[  101,  9805,  3540, 11514,  2050,  3079, 11282,  2243,  1005,  1055,\n          2077,  4855,  1996,  4677,  2000,  3647,  4576,  1999,  2687,  2005,\n          1002,  1016,  1012,  1019,  4551,  1012,   102,  9805,  3540, 11514,\n          2050,  4149, 11282,  2243,  1005,  1055,  1999,  2786,  2005,  1002,\n          6353,  2509,  2454,  1998,  2853,  2009,  2000,  3647,  4576,  2005,\n          1002,  1015,  1012,  1022,  4551,  1999,  2687,  1012,   102]])\nAttention Mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\nToken Type IDs : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\nExemple 3:\nPhrase 1 : They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\nPhrase 2 : On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\nTokens : tensor([[  101,  2027,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  2006,\n          2238,  2184,  1010,  5378,  1996,  6636,  2005,  5096,  1010,  2002,\n          2794,  1012,   102,  2006,  2238,  2184,  1010,  1996,  2911,  1005,\n          1055,  5608,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  1010,\n          5378,  1996, 14792,  2005,  5096,  1012,   102]])\nAttention Mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\nToken Type IDs : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":329},{"cell_type":"markdown","source":"Utilisez la fonction decode du tokenizer afin d'avoir une représentation plus lisible du résultat de tokenisation. Vous remarquerez l'ajout des tokens [CLS], [SEP] notamment.","metadata":{"id":"hjUYsmhjXkFw"}},{"cell_type":"code","source":"# Tokeniser quelques exemples du dataset\ntokenized_example_1 = tokenizer(\n    \"Amrozi accused his brother, whom he called 'the witness', of deliberately distorting his evidence.\",\n    padding='max_length', truncation=True, return_tensors='pt'\n)\ntokenized_example_2 = tokenizer(\n    \"Referring to him as only 'the witness', Amrozi accused his brother of deliberately distorting his evidence.\",\n    padding='max_length', truncation=True, return_tensors='pt'\n)\n\n# Utilisation de la fonction decode pour afficher une représentation lisible\ndecoded_example_1 = tokenizer.decode(\n    tokenized_example_1[\"input_ids\"][0], skip_special_tokens=False\n)\ndecoded_example_2 = tokenizer.decode(\n    tokenized_example_2[\"input_ids\"][0], skip_special_tokens=False\n)\n\nprint(\"Phrase tokenisée et décodée - Exemple 1 :\")\nprint(decoded_example_1)\nprint(\"\\nPhrase tokenisée et décodée - Exemple 2 :\")\nprint(decoded_example_2)\n","metadata":{"id":"_cvcdwSXX6Bt","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.821102Z","iopub.execute_input":"2024-11-20T15:31:08.821434Z","iopub.status.idle":"2024-11-20T15:31:08.834066Z","shell.execute_reply.started":"2024-11-20T15:31:08.821401Z","shell.execute_reply":"2024-11-20T15:31:08.833116Z"}},"outputs":[{"name":"stdout","text":"Phrase tokenisée et décodée - Exemple 1 :\n[CLS] amrozi accused his brother, whom he called ' the witness ', of deliberately distorting his evidence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\nPhrase tokenisée et décodée - Exemple 2 :\n[CLS] referring to him as only ' the witness ', amrozi accused his brother of deliberately distorting his evidence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","output_type":"stream"}],"execution_count":330},{"cell_type":"markdown","source":"Tokenisez tout le dataset.\n\nConseil : Utilisez la fonction map de la library datasets.","metadata":{"id":"7Rr1jeb9X6Ok"}},{"cell_type":"code","source":"# Tokenisation de tout le dataset avec la fonction map\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=True, return_tensors=\"pt\",max_length=65)\n\n# Application de la fonction de tokenisation\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n# Affichage des clés du dataset tokenisé pour confirmation\nprint(tokenized_dataset)\n","metadata":{"id":"npPFpk2DY4jr","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.835032Z","iopub.execute_input":"2024-11-20T15:31:08.835329Z","iopub.status.idle":"2024-11-20T15:31:08.877460Z","shell.execute_reply.started":"2024-11-20T15:31:08.835297Z","shell.execute_reply":"2024-11-20T15:31:08.876542Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1725\n    })\n})\n","output_type":"stream"}],"execution_count":331},{"cell_type":"markdown","source":"Une fois le dataset tokenisé, il faut créer un DataLoader qui va donner les exemples au modèle durant l'entraînement.   \nDéfinissez un train_dataloader et un eval_dataloader.  \n\nPour le train_dataloader, spécifiez `shuffle=True` afin que les exemples soient donnés dans un ordre aléatoire au modèle.\n\nIl est à noter que le modèle s'attend à prendre en entrée des exemples sous forme de **dictionnaires** de **tenseurs torch** et ayant les clés suivantes uniquement:  \n- attention_mask  \n- input_ids  \n- labels  \n- token_type_ids  \n\nVous aurez probablement à définir une fonction de collate à donner en paramètre à vos dataloaders dans le paramètre `collate_fn`.  Cette fonction est appelée par le dataloader juste avant de donner l'exemple au modèle afin de lui donner le bon format (mettre uniquement les clés attendues par le modèle, padder les exemples à une longueur fixe, ...).  \n\n- [How to use collate_fn](https://discuss.pytorch.org/t/how-to-use-collate-fn/27181)  \n- [How to use 'collate_fn' with dataloaders?](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)  ","metadata":{"id":"UO0-1VvJY1uG"}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch\n\n# 1. Dataset personnalisé\nclass CustomDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.tokenized_data = tokenized_data  # Votre dataset tokenisé\n\n    def __len__(self):\n        return len(self.tokenized_data)\n\n    def __getitem__(self, idx):\n        return self.tokenized_data[idx]\n        \n# Séparer le dataset en train et évaluation\ntrain_dataset = tokenized_dataset['train']  # Données d'entraînement\neval_dataset = tokenized_dataset['validation']  # Données de validation\n\n# 2. Fonction collate_fn\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    # Définir la longueur maximale pour le padding\n    max_len = 65  # La longueur maximale que vous souhaitez pour les séquences\n    \n    input_ids = [torch.tensor(item['input_ids'][:max_len]) for item in batch]\n    attention_masks = [torch.tensor(item['attention_mask'][:max_len]) for item in batch]\n    token_type_ids = [torch.tensor(item['token_type_ids'][:max_len]) for item in batch]\n    \n    labels = [torch.tensor(item['label']) for item in batch]\n\n    # Padding à la longueur maximale\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n    token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n    \n    batch_dict = {\n        'input_ids': input_ids,\n        'attention_mask': attention_masks,\n        'labels': labels,\n        'token_type_ids': token_type_ids\n\n    }\n    \n    if labels is not None:\n        batch_dict['labels'] = torch.stack(labels)\n\n    return batch_dict\n\n\n\n\n# 3. Charger votre dataset tokenisé (supposons que vous avez `train_dataset` et `eval_dataset` déjà définis)\ntrain_dataset = CustomDataset(train_dataset)  # Votre dataset tokenisé pour l'entraînement\neval_dataset = CustomDataset(eval_dataset)    # Votre dataset tokenisé pour l'évaluation\n\n# 4. Créer les DataLoader\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=True,\n    collate_fn=collate_fn\n)\n\neval_dataloader = DataLoader(\n    eval_dataset,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=collate_fn\n)\n","metadata":{"id":"c26Wa7-OZh2T","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.878875Z","iopub.execute_input":"2024-11-20T15:31:08.879195Z","iopub.status.idle":"2024-11-20T15:31:08.895640Z","shell.execute_reply.started":"2024-11-20T15:31:08.879161Z","shell.execute_reply":"2024-11-20T15:31:08.894441Z"}},"outputs":[],"execution_count":332},{"cell_type":"markdown","source":"Afin de vérifier que votre Dataloader fonctionne bien, vous pouvez itérer dessus comme ci-dessous :","metadata":{"id":"ynCNm7VeY54_"}},{"cell_type":"code","source":"for batch in train_dataloader:\n    break\n{k: v.shape for k, v in batch.items()}","metadata":{"id":"f5mISj69c07Z","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.897117Z","iopub.execute_input":"2024-11-20T15:31:08.897422Z","iopub.status.idle":"2024-11-20T15:31:08.917247Z","shell.execute_reply.started":"2024-11-20T15:31:08.897389Z","shell.execute_reply":"2024-11-20T15:31:08.916226Z"}},"outputs":[{"execution_count":333,"output_type":"execute_result","data":{"text/plain":"{'input_ids': torch.Size([8, 65]),\n 'attention_mask': torch.Size([8, 65]),\n 'labels': torch.Size([8]),\n 'token_type_ids': torch.Size([8, 65])}"},"metadata":{}}],"execution_count":333},{"cell_type":"markdown","source":"Le code ne devrait pas crasher et vous devriez avoir une output similaire à:  \n```\n{\n  'attention_mask': torch.Size([8, 65]),\n  'input_ids': torch.Size([8, 65]),\n  'labels': torch.Size([8]),\n  'token_type_ids': torch.Size([8, 65])\n}\n```","metadata":{"id":"_jUpAq0QdBF8"}},{"cell_type":"markdown","source":"Vous pouvez vous assurer que le modèle prend en entrée les données du dataloader en lui donnant en entrée le batch précédent. Voici un exemple de code:  ","metadata":{"id":"fkBg5LLJdjoi"}},{"cell_type":"code","source":"outputs = model(**batch)\nprint(outputs.loss, outputs.logits.shape)","metadata":{"id":"Xt5VbJ_dd2Lc","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:08.918416Z","iopub.execute_input":"2024-11-20T15:31:08.918721Z","iopub.status.idle":"2024-11-20T15:31:09.521225Z","shell.execute_reply.started":"2024-11-20T15:31:08.918690Z","shell.execute_reply":"2024-11-20T15:31:09.520243Z"}},"outputs":[{"name":"stdout","text":"tensor(0.4949, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n","output_type":"stream"}],"execution_count":334},{"cell_type":"markdown","source":"Maintenant que tout est en place, nous allons lancer l'entraînement.  \nDéfinissez l'optimizer à utiliser durant l'apprentissage. Par exemple AdamW.  \nDéfinissez aussi les paramètres d'apprentissage, par exemple:  \n- Nombre d'epochs  \n- Nombre de training steps  \n- Un schedule pour le learning rate ou un learning rate fixe  ","metadata":{"id":"yPSrtCtsd57K"}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n# Définir l'optimiseur\noptimizer = AdamW(model.parameters(), lr=2e-5)\n# Nombre d'époques et de training steps\nnum_epochs = 3\nnum_training_steps = len(train_dataloader) * num_epochs\n\n\n# Définir un scheduler de learning rate avec warm-up\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,  # Nombre d'étapes pour le warm-up (souvent entre 0 et 10% des étapes totales)\n    num_training_steps=num_training_steps\n)\n","metadata":{"id":"7tPpK-lueeIA","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:09.524279Z","iopub.execute_input":"2024-11-20T15:31:09.524614Z","iopub.status.idle":"2024-11-20T15:31:09.530858Z","shell.execute_reply.started":"2024-11-20T15:31:09.524587Z","shell.execute_reply":"2024-11-20T15:31:09.529812Z"}},"outputs":[],"execution_count":335},{"cell_type":"markdown","source":"\nVous pouvez aussi utiliser le Trainer de la library transformers.  ","metadata":{"id":"V8ISzCW9edYX"}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score\n\n# Définir la fonction de calcul des métriques\ndef compute_metrics(p):\n    preds = p.predictions.argmax(axis=1)  # Prédictions en prenant l'index avec la probabilité la plus élevée\n    labels = p.label_ids\n    return {\"eval_accuracy\": accuracy_score(labels, preds)}  # Renvoie eval_accuracy comme clé\n\n# Définir les arguments d'entraînement\ntraining_args = TrainingArguments(\n    output_dir='./results',             # Répertoire pour sauvegarder les résultats\n    num_train_epochs=3,                 # Nombre d'époques\n    per_device_train_batch_size=8,      # Taille du batch pour l'entraînement\n    per_device_eval_batch_size=8,       # Taille du batch pour l'évaluation\n    warmup_steps=0,                     # Nombre de warm-up steps\n    weight_decay=0.01,                  # Poids pour la régularisation L2\n    logging_dir='./logs',               # Répertoire des logs\n    logging_steps=10,                   # Fréquence de logging\n    evaluation_strategy=\"epoch\",        # Évaluation après chaque époque\n    save_strategy=\"epoch\",              # Sauvegarde après chaque époque\n    load_best_model_at_end=True,        # Charger le meilleur modèle à la fin\n    metric_for_best_model=\"eval_accuracy\",  # Critère pour sélectionner le meilleur modèle\n    report_to=\"none\"                   # Pas de rapport vers W&B\n)\n\n# Créer un objet Trainer\ntrainer = Trainer(\n    model=model,                        # Modèle à entraîner\n    args=training_args,                 # Les arguments d'entraînement\n    train_dataset=train_dataset,         # Dataset d'entraînement\n    eval_dataset=eval_dataset,           # Dataset d'évaluation\n    tokenizer=tokenizer,                 # Tokenizer\n    compute_metrics=compute_metrics,     # Fonction pour calculer les métriques\n    optimizers=(optimizer, lr_scheduler) # Optimiseur et scheduler\n)\n\n# Lancer l'entraînement\ntrainer.train()\n\n","metadata":{"id":"wlB1fr9feo9K","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:31:09.532186Z","iopub.execute_input":"2024-11-20T15:31:09.532872Z","iopub.status.idle":"2024-11-20T15:33:19.099243Z","shell.execute_reply.started":"2024-11-20T15:31:09.532831Z","shell.execute_reply":"2024-11-20T15:33:19.098325Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1377/1377 02:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.561500</td>\n      <td>0.368206</td>\n      <td>0.843137</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.238100</td>\n      <td>0.400536</td>\n      <td>0.860294</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.026400</td>\n      <td>0.518481</td>\n      <td>0.867647</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":336,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1377, training_loss=0.32814860387218975, metrics={'train_runtime': 128.8769, 'train_samples_per_second': 85.384, 'train_steps_per_second': 10.685, 'total_flos': 367564088782800.0, 'train_loss': 0.32814860387218975, 'epoch': 3.0})"},"metadata":{}}],"execution_count":336},{"cell_type":"markdown","source":"Ecrivez la boucle d'évaluation de votre modèle. Pour cela, vous pouvez utiliser la library evaluate de huggingface qui facilite le calcul des métriques. Elle contient la métrique spécifique à la tâche qui nous intéresse. Voici un exemple de comment la charger :  \n\nVoici la page associée pour avoir de la doc sur les métriques GLUE : [Metric: glue](https://huggingface.co/spaces/evaluate-metric/glue)","metadata":{"id":"oZZtN8L8ZjtJ"}},{"cell_type":"code","source":"import torch\nimport evaluate\nfrom tqdm import tqdm\n\n# Charger la métrique (par exemple, accuracy)\naccuracy_metric = evaluate.load(\"accuracy\")\n\n# Définir le périphérique (GPU ou CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Fonction d'évaluation\ndef evaluate_model(model, dataloader, device):\n    model.eval()  # Mettre le modèle en mode évaluation\n    all_preds = []\n    all_labels = []\n    \n    # Déplacer le modèle vers le bon périphérique\n    model.to(device)\n    \n    # Désactivation de la mise à jour des gradients\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            # Déplacer les données vers le bon périphérique (GPU ou CPU)\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Effectuer une passe avant (forward pass)\n            outputs = model(**batch)\n            logits = outputs.logits\n            \n            # Convertir les logits en prédictions de classes\n            preds = torch.argmax(logits, dim=-1)\n            \n            # Ajouter les prédictions et labels à leurs listes respectives\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n    \n    # Calcul de la métrique (ex. : accuracy)\n    results = accuracy_metric.compute(predictions=all_preds, references=all_labels)\n    \n    return results\n\n# Appel de la fonction d'évaluation sur votre modèle\nresults = evaluate_model(model, eval_dataloader, device)\nprint(\"Evaluation Results:\", results)\n","metadata":{"id":"MB-Nq-cufbQU","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:38:56.346900Z","iopub.execute_input":"2024-11-20T15:38:56.347711Z","iopub.status.idle":"2024-11-20T15:38:57.744673Z","shell.execute_reply.started":"2024-11-20T15:38:56.347675Z","shell.execute_reply":"2024-11-20T15:38:57.743806Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 51/51 [00:01<00:00, 45.31it/s]","output_type":"stream"},{"name":"stdout","text":"Evaluation Results: {'accuracy': 0.8676470588235294}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":341},{"cell_type":"markdown","source":"# [Optionnel] Génération de texte avec GPT2\n\nSi vous souhaitez expérimenter avec un language model est faire de la génération de texte. Vous pouvez utiliser le modèle GPT 2 qui est un language model causal. Il permet donc d'effectuer de la génération de texte contrairement à BERT qui est un modèle bidirectionnel et qui n'est pas fait pour cela.  \n\nVous pouvez essayer différents paramètres de génération et algorithme de décodage en vous basant sur le tutoriel/guide suivant : [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate).","metadata":{"id":"bzTmAOFeuM7X"}},{"cell_type":"code","source":"","metadata":{"id":"4HR0z0M3vG0A","trusted":true},"outputs":[],"execution_count":null}]}
